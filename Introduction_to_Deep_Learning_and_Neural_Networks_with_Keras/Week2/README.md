# Artificial Neural Networks

In this module, you will learn about the gradient descent algorithm and how variables are optimized with respect to a defined function. You will also learn about backpropagation and how neural networks learn and update their weights and biases. Futhermore, you will learn about the vanishing gradient problem. Finally, you will learn about activation functions.

## Learning Objectives

- Describe the gradient descent algorithm and how variables are optimized with respect to a defined function.
- Define backpropagation and how neural networks learn and update their weights and biases.
- Explain the vanishing gradient problem.
- Define activation functions.

## Training a Neural Network

- [Video - Gradient Descent](https://www.coursera.org/learn/introduction-to-deep-learning-with-keras/lecture/xabij/gradient-descent)

- [Video - Backpropagation](https://www.coursera.org/learn/introduction-to-deep-learning-with-keras/lecture/lfbxX/backpropagation)

- [Video - Vanishing Gradient](https://www.coursera.org/learn/introduction-to-deep-learning-with-keras/lecture/1BYiq/vanishing-gradient)

- [Video - Activation Functions](https://www.coursera.org/learn/introduction-to-deep-learning-with-keras/lecture/tL4uI/activation-functions)