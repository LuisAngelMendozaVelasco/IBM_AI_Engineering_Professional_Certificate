{"cells":[{"cell_type":"markdown","id":"66f07cc3-f171-4267-aee6-ce7752933b77","metadata":{},"source":["<a href=\"https://cognitiveclass.ai\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/image/IDSN-logo.png\" width=\"400\"> </a>\n","\n","<h1 align=center><font size = 5>Pre-Trained Models</font></h1>\n"]},{"cell_type":"markdown","id":"e331f40b-337d-4c41-9371-04596800ad74","metadata":{},"source":["## Objective\n"]},{"cell_type":"markdown","id":"48dc60a8-2320-435c-90e1-f7b44c170728","metadata":{},"source":["In this lab, you will learn how to leverage pre-trained models to build image classifiers instead of building a model from scratch.\n"]},{"cell_type":"markdown","id":"ac02d0d3-1136-426e-8294-0d7d9d505786","metadata":{},"source":["## Table of Contents\n","\n","<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n","\n","<font size = 3> \n","    \n","1. <a href=\"#item31\">Import Libraries and Packages</a>\n","2. <a href=\"#item32\">Download Data</a>  \n","3. <a href=\"#item33\">Define Global Constants</a>  \n","4. <a href=\"#item34\">Construct ImageDataGenerator Instances</a>  \n","5. <a href=\"#item35\">Compile and Fit Model</a>\n","\n","</font>\n","    \n","</div>\n"]},{"cell_type":"markdown","id":"1a39a4dc-1c2c-4bf1-bedb-eac3969c838a","metadata":{},"source":["   \n"]},{"cell_type":"markdown","id":"5444ae91-6e40-4287-a194-bcb2d269ed50","metadata":{},"source":["<a id='item31'></a>\n"]},{"cell_type":"markdown","id":"9008da8d-7cb4-4333-925d-9937c5a83ab6","metadata":{},"source":["## Import Libraries and Packages\n"]},{"cell_type":"markdown","id":"d0ff4a4c-47c2-4372-8c59-2cfda80d64e7","metadata":{},"source":["Let's start the lab by importing the libraries that we will be using in this lab. First we will need the library that helps us to import the data.\n"]},{"cell_type":"code","execution_count":null,"id":"2cc2be08-b540-4918-b1a9-a2df287ff4d9","metadata":{},"outputs":[],"source":["# import skillsnetwork "]},{"cell_type":"markdown","id":"9a683d5c-10fd-4e9b-9fe7-3cd2c8928099","metadata":{},"source":["First, we will import the ImageDataGenerator module since we will be leveraging it to train our model in batches.\n"]},{"cell_type":"code","execution_count":1,"id":"675684c0-2392-4487-953a-99b049b0450c","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-02-27 23:27:22.162586: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"]}],"source":["from keras.preprocessing.image import ImageDataGenerator"]},{"cell_type":"markdown","id":"683598d4-99e2-42b0-991e-00a3e5fd5d99","metadata":{},"source":["In this lab, we will be using the Keras library to build an image classifier, so let's download the Keras library.\n"]},{"cell_type":"code","execution_count":2,"id":"58f57fd4-ffb5-4fd7-a67c-0cf73a1cacde","metadata":{},"outputs":[],"source":["import keras\n","from keras.models import Sequential\n","from keras.layers import Dense"]},{"cell_type":"markdown","id":"3ecf765d-36f8-4262-b631-cb0f60a217d9","metadata":{},"source":["Finally, we will be leveraging the ResNet50 model to build our classifier, so let's download it as well.\n"]},{"cell_type":"code","execution_count":4,"id":"f96d6994-63e4-4297-b98a-0c2ab2439cd0","metadata":{},"outputs":[],"source":["from keras.applications import ResNet50\n","from keras.applications.resnet import preprocess_input"]},{"cell_type":"markdown","id":"53d76782-0cd7-4ced-8a37-096675e632d3","metadata":{},"source":["<a id='item32'></a>\n"]},{"cell_type":"markdown","id":"9e55f048-ce67-455a-9e3a-dc5cc0ded766","metadata":{},"source":["## Download Data\n"]},{"cell_type":"markdown","id":"89eae3c7-10fc-4e55-a043-b544dd7776f4","metadata":{},"source":["In this section, you are going to download the data from IBM object storage using **skillsnetwork.prepare** command. skillsnetwork.prepare is a command that's used to download a zip file, unzip it and store it in a specified directory.\n"]},{"cell_type":"code","execution_count":7,"id":"24ee9fe4-8e25-45a5-b56b-61c51b69bbb5","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["--2024-02-27 23:37:16--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/concrete_data_week3.zip\n","Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104\n","Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 97863179 (93M) [application/zip]\n","Saving to: ‘data/concrete_data_week3.zip’\n","\n","concrete_data_week3 100%[===================>]  93.33M  4.75MB/s    in 16s     \n","\n","2024-02-27 23:37:33 (5.69 MB/s) - ‘data/concrete_data_week3.zip’ saved [97863179/97863179]\n","\n"]}],"source":["## get the data\n","# await skillsnetwork.prepare(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/concrete_data_week3.zip\", overwrite=True)\n","!mkdir data\n","!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/concrete_data_week3.zip -P data/\n","!unzip -qq ./data/concrete_data_week3.zip -d data/"]},{"cell_type":"markdown","id":"d1926133-f936-42c7-8cec-7e5ebcbb20b9","metadata":{},"source":["Now, you should see the folder *concrete_data_week3* appear in the left pane. If you open this folder by double-clicking on it, you will find that it contains two folders: *train* and *valid*. And if you explore these folders, you will find that each contains two subfolders: *positive* and *negative*. These are the same folders that we saw in the labs in the previous modules of this course, where *negative* is the negative class and it represents the concrete images with no cracks and *positive* is the positive class and it represents the concrete images with cracks.\n"]},{"cell_type":"markdown","id":"6d1c1123-f5b8-4270-aa89-27c71ca7d4a9","metadata":{},"source":["**Important Note**: There are thousands and thousands of images in each folder, so please don't attempt to double click on the *negative* and *positive* folders. This may consume all of your memory and you may end up with a **50** error. So please **DO NOT DO IT**.\n"]},{"cell_type":"markdown","id":"ddd1dd78-ec92-4710-8cdf-47091a74a872","metadata":{},"source":["<a id='item33'></a>\n"]},{"cell_type":"markdown","id":"877483a0-cb76-4928-a36a-39785cf04808","metadata":{},"source":["## Define Global Constants\n"]},{"cell_type":"markdown","id":"b192844f-8ebf-485b-b97d-e525515fa5d2","metadata":{},"source":["Here, we will define constants that we will be using throughout the rest of the lab. \n","\n","1. We are obviously dealing with two classes, so *num_classes* is 2. \n","2. The ResNet50 model was built and trained using images of size (224 x 224). Therefore, we will have to resize our images from (227 x 227) to (224 x 224).\n","3. We will training and validating the model using batches of 100 images.\n"]},{"cell_type":"code","execution_count":8,"id":"b5f7cace-b3c4-4e58-9c0d-5b52f3357713","metadata":{},"outputs":[],"source":["num_classes = 2\n","image_resize = 224\n","batch_size_training = 100\n","batch_size_validation = 100"]},{"cell_type":"markdown","id":"0183cdad-017f-41ea-ac37-d979b398ad16","metadata":{},"source":["<a id='item34'></a>\n"]},{"cell_type":"markdown","id":"cac7d34d-9a92-44de-8245-d22202b73b3a","metadata":{},"source":["## Construct ImageDataGenerator Instances\n"]},{"cell_type":"markdown","id":"cd27dd04-a55f-4c99-9259-0b2c3d296090","metadata":{},"source":["In order to instantiate an ImageDataGenerator instance, we will set the **preprocessing_function** argument to *preprocess_input* which we imported from **keras.applications.resnet50** in order to preprocess our images the same way the images used to train ResNet50 model were processed.\n"]},{"cell_type":"code","execution_count":9,"id":"db7834eb-a1e4-4a4b-a075-29900939bdc4","metadata":{},"outputs":[],"source":["data_generator = ImageDataGenerator(\n","    preprocessing_function=preprocess_input,\n",")"]},{"cell_type":"markdown","id":"6b487ed3-ae96-4b96-8038-7e5cda19b36c","metadata":{},"source":["Next, we will use the *flow_from_directory* method to get the training images as follows:\n"]},{"cell_type":"code","execution_count":10,"id":"595ecbe2-bfdc-4523-9acd-3706181796b8","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 10001 images belonging to 2 classes.\n"]}],"source":["train_generator = data_generator.flow_from_directory(\n","    './data/concrete_data_week3/train',\n","    target_size=(image_resize, image_resize),\n","    batch_size=batch_size_training,\n","    class_mode='categorical')"]},{"cell_type":"markdown","id":"83e1f3fd-c370-4b6a-a178-2299a39880cb","metadata":{},"source":["**Note**: in this lab, we will be using the full data-set of 40,000 images for training and validation.\n"]},{"cell_type":"markdown","id":"160bbbd7-035b-4c35-b870-556433bcd577","metadata":{},"source":["**Your Turn**: Use the *flow_from_directory* method to get the validation images and assign the result to **validation_generator**.\n"]},{"cell_type":"code","execution_count":12,"id":"cc0f0817-76c4-4467-a062-c0c302658371","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 5001 images belonging to 2 classes.\n"]}],"source":["## Type your answer here\n","\n","validation_generator = data_generator.flow_from_directory(\n","    './data/concrete_data_week3/valid',\n","    target_size=(image_resize, image_resize),\n","    batch_size=batch_size_validation,\n","    class_mode='categorical')"]},{"cell_type":"markdown","id":"60a0dc3d-3401-4b13-8237-fef3435998a7","metadata":{},"source":["Double-click __here__ for the solution.\n","<!-- The correct answer is:\n","validation_generator = data_generator.flow_from_directory(\n","    'concrete_data_week3/valid',\n","    target_size=(image_resize, image_resize),\n","    batch_size=batch_size_validation,\n","    class_mode='categorical')\n","-->\n","\n"]},{"cell_type":"markdown","id":"9cefbac4-f731-44a4-b928-c53b635b687f","metadata":{},"source":["<a id='item35'></a>\n"]},{"cell_type":"markdown","id":"b1e87236-bfd3-4ff8-942d-133ebd71d2d2","metadata":{},"source":["## Build, Compile and Fit Model\n"]},{"cell_type":"markdown","id":"4f398574-9232-4051-8463-663fe90a2f47","metadata":{},"source":["In this section, we will start building our model. We will use the Sequential model class from Keras.\n"]},{"cell_type":"code","execution_count":13,"id":"324b72dc-2a07-48a6-a4e7-b91de8338dcd","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-02-27 23:41:02.134268: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"]}],"source":["model = Sequential()"]},{"cell_type":"markdown","id":"5f9ce8f4-3052-478f-92e0-412574a9ffba","metadata":{},"source":["Next, we will add the ResNet50 pre-trained model to out model. However, note that we don't want to include the top layer or the output layer of the pre-trained model. We actually want to define our own output layer and train it so that it is optimized for our image dataset. In order to leave out the output layer of the pre-trained model, we will use the argument *include_top* and set it to **False**.\n"]},{"cell_type":"code","execution_count":14,"id":"4f75d767-8ca1-4cea-b11f-e8c73909ef35","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n","94765736/94765736 [==============================] - 14s 0us/step\n"]}],"source":["model.add(ResNet50(\n","    include_top=False,\n","    pooling='avg',\n","    weights='imagenet',\n","    ))"]},{"cell_type":"markdown","id":"5fa6e617-2400-470a-aad9-416bdc45f07b","metadata":{},"source":["Then, we will define our output layer as a **Dense** layer, that consists of two nodes and uses the **Softmax** function as the activation function.\n"]},{"cell_type":"code","execution_count":15,"id":"517f94df-1874-457c-84e6-e1dba49f86e0","metadata":{},"outputs":[],"source":["model.add(Dense(num_classes, activation='softmax'))"]},{"cell_type":"markdown","id":"1dd8b80d-795d-41cc-897f-d942bb8ceaa5","metadata":{},"source":["You can access the model's layers using the *layers* attribute of our model object. \n"]},{"cell_type":"code","execution_count":16,"id":"7c3e20f7-ccd7-42aa-8a95-5bd0e5361899","metadata":{},"outputs":[{"data":{"text/plain":["[<keras.engine.functional.Functional at 0x7f1504fd5150>,\n"," <keras.layers.core.dense.Dense at 0x7f1504fe1310>]"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["model.layers"]},{"cell_type":"markdown","id":"ead62664-0389-4b64-a87b-31054cae45f1","metadata":{},"source":["You can see that our model is composed of two sets of layers. The first set is the layers pertaining to ResNet50 and the second set is a single layer, which is our Dense layer that we defined above.\n"]},{"cell_type":"markdown","id":"bfb56e7f-9153-4ecb-a22f-34e4a82c3a1e","metadata":{},"source":["You can access the ResNet50 layers by running the following:\n"]},{"cell_type":"code","execution_count":17,"id":"3adb56a8-130d-4e65-8f77-f015cf759afa","metadata":{},"outputs":[{"data":{"text/plain":["[<keras.engine.input_layer.InputLayer at 0x7f152637b010>,\n"," <keras.layers.reshaping.zero_padding2d.ZeroPadding2D at 0x7f15283c0250>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x7f152635ce10>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f15263707d0>,\n"," <keras.layers.core.activation.Activation at 0x7f15283c14d0>,\n"," <keras.layers.reshaping.zero_padding2d.ZeroPadding2D at 0x7f15283bc210>,\n"," <keras.layers.pooling.max_pooling2d.MaxPooling2D at 0x7f1526373f50>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x7f15263a2e50>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f1505656b90>,\n"," <keras.layers.core.activation.Activation at 0x7f15283c1350>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x7f152635d710>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f15283b8a50>,\n"," <keras.layers.core.activation.Activation at 0x7f152ed80750>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x7f1505655d50>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x7f1528a25a50>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f15264c3710>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f152f44ca10>,\n"," <keras.layers.merging.add.Add at 0x7f1526371d50>,\n"," <keras.layers.core.activation.Activation at 0x7f152635f4d0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x7f1505672310>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f152637a650>,\n"," <keras.layers.core.activation.Activation at 0x7f15264c2e90>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x7f1505671a10>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f15283b96d0>,\n"," <keras.layers.core.activation.Activation at 0x7f1526373c50>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x7f1505672a50>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f1528a25d10>,\n"," <keras.layers.merging.add.Add at 0x7f152635f950>,\n"," <keras.layers.core.activation.Activation at 0x7f15283a6c50>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x7f150568e6d0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f1505695750>,\n"," <keras.layers.core.activation.Activation at 0x7f152638ed50>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x7f15056a7410>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f15056a79d0>,\n"," <keras.layers.core.activation.Activation at 0x7f1582648250>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x7f15056a7010>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f15056b3f90>,\n"," <keras.layers.merging.add.Add at 0x7f150565d7d0>,\n"," <keras.layers.core.activation.Activation at 0x7f152637a490>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x7f15056972d0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f15283c0d50>,\n"," <keras.layers.core.activation.Activation at 0x7f1526339790>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x7f15056d1550>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f152633be10>,\n"," <keras.layers.core.activation.Activation at 0x7f15056d0b10>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x7f152637bf50>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x7f15056c4950>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f1505673810>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f15056c3490>,\n"," <keras.layers.merging.add.Add at 0x7f1505696c50>,\n"," <keras.layers.core.activation.Activation at 0x7f152635d950>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x7f150569cb90>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f150569f990>,\n"," <keras.layers.core.activation.Activation at 0x7f152a1e7150>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x7f150568dc50>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f15056c4850>,\n"," <keras.layers.core.activation.Activation at 0x7f1529040090>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x7f150568c290>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f15056c6950>,\n"," <keras.layers.merging.add.Add at 0x7f1526327e50>,\n"," <keras.layers.core.activation.Activation at 0x7f15056c3690>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x7f15056f46d0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f15263a0bd0>,\n"," <keras.layers.core.activation.Activation at 0x7f1505673f90>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x7f15056f7850>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f15056f7d50>,\n"," <keras.layers.core.activation.Activation at 0x7f15056f4490>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x7f1504eff090>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f1505672950>,\n"," <keras.layers.merging.add.Add at 0x7f152eda4fd0>,\n"," <keras.layers.core.activation.Activation at 0x7f15056d9890>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x7f15056c7f50>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f1504f0f990>,\n"," <keras.layers.core.activation.Activation at 0x7f15056f5690>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x7f1504f16d10>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f15056b1090>,\n"," <keras.layers.core.activation.Activation at 0x7f1526390650>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x7f1504f27e90>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f1528a25b90>,\n"," <keras.layers.merging.add.Add at 0x7f1505694790>,\n"," <keras.layers.core.activation.Activation at 0x7f1504f148d0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x7f1504f0f890>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f15056d1810>,\n"," <keras.layers.core.activation.Activation at 0x7f15056a4710>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x7f15056d9bd0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f1504f0cf50>,\n"," <keras.layers.core.activation.Activation at 0x7f1504f242d0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x7f1504f3a3d0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x7f1504f38350>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f1504f0e1d0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f1526366490>,\n"," <keras.layers.merging.add.Add at 0x7f15056c2490>,\n"," <keras.layers.core.activation.Activation at 0x7f15056db710>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x7f1505656010>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f15283bae10>,\n"," <keras.layers.core.activation.Activation at 0x7f1504f311d0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x7f1504f47490>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f1504f26bd0>,\n"," <keras.layers.core.activation.Activation at 0x7f15263936d0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x7f1504f52910>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f15283c36d0>,\n"," <keras.layers.merging.add.Add at 0x7f1504f61f90>,\n"," <keras.layers.core.activation.Activation at 0x7f1504f61c10>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x7f1504f0b7d0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f1504f67d10>,\n"," <keras.layers.core.activation.Activation at 0x7f1504f46a90>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x7f1504f67510>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f1504f61190>,\n"," <keras.layers.core.activation.Activation at 0x7f1504f71950>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x7f1504f70310>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f1504f26610>,\n"," <keras.layers.merging.add.Add at 0x7f1529ce8e90>,\n"," <keras.layers.core.activation.Activation at 0x7f1504f846d0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x7f1504f98fd0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f1504f69110>,\n"," <keras.layers.core.activation.Activation at 0x7f1504f32510>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x7f1504f9b0d0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f1504f9b550>,\n"," <keras.layers.core.activation.Activation at 0x7f15268b6650>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x7f1504f99310>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f1504fa9250>,\n"," <keras.layers.merging.add.Add at 0x7f1504fb01d0>,\n"," <keras.layers.core.activation.Activation at 0x7f1504fb2510>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x7f1504f87950>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f1505657850>,\n"," <keras.layers.core.activation.Activation at 0x7f15283a61d0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x7f1504f859d0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f1504f0d450>,\n"," <keras.layers.core.activation.Activation at 0x7f1504f3be10>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x7f1504f31e50>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f1504fa8b50>,\n"," <keras.layers.merging.add.Add at 0x7f15283ceed0>,\n"," <keras.layers.core.activation.Activation at 0x7f1504f9acd0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x7f1504f69850>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f1504f6ae90>,\n"," <keras.layers.core.activation.Activation at 0x7f1504f52310>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x7f1504f70a90>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f1504f141d0>,\n"," <keras.layers.core.activation.Activation at 0x7f1504f51890>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x7f1504f6b850>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f1504fc1e90>,\n"," <keras.layers.merging.add.Add at 0x7f1504f52550>,\n"," <keras.layers.core.activation.Activation at 0x7f1504f71dd0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x7f1504fe3d90>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f1504fd7dd0>,\n"," <keras.layers.core.activation.Activation at 0x7f1504fdb590>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x7f1504ff2990>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f1504fea7d0>,\n"," <keras.layers.core.activation.Activation at 0x7f15284f3550>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x7f1504fdbd50>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x7f1504ff1e90>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f1504fdbed0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f15043fb7d0>,\n"," <keras.layers.merging.add.Add at 0x7f15043fb950>,\n"," <keras.layers.core.activation.Activation at 0x7f15043fb050>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x7f1504408c50>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f152635fd10>,\n"," <keras.layers.core.activation.Activation at 0x7f1504407510>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x7f150440be90>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f150440b450>,\n"," <keras.layers.core.activation.Activation at 0x7f152638e710>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x7f1504409350>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f1504410550>,\n"," <keras.layers.merging.add.Add at 0x7f15044050d0>,\n"," <keras.layers.core.activation.Activation at 0x7f1504404b50>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x7f1504fda690>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f1504f45890>,\n"," <keras.layers.core.activation.Activation at 0x7f1504f244d0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x7f1504f69010>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f1504fb1050>,\n"," <keras.layers.core.activation.Activation at 0x7f1504f83090>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x7f1504f864d0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f1504412ed0>,\n"," <keras.layers.merging.add.Add at 0x7f15056da190>,\n"," <keras.layers.core.activation.Activation at 0x7f1504faa1d0>,\n"," <keras.layers.pooling.global_average_pooling2d.GlobalAveragePooling2D at 0x7f1504fa7d10>]"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["model.layers[0].layers"]},{"cell_type":"markdown","id":"bcc55835-9b6a-4270-a922-ffc842a3b2f7","metadata":{},"source":["Since the ResNet50 model has already been trained, then we want to tell our model not to bother with training the ResNet part, but to train only our dense output layer. To do that, we run the following.\n"]},{"cell_type":"code","execution_count":18,"id":"ba503665-a9ca-456c-8a6a-072767030476","metadata":{},"outputs":[],"source":["model.layers[0].trainable = False"]},{"cell_type":"markdown","id":"eb8c4bc5-43b0-41af-84b8-c3f44700a241","metadata":{},"source":["And now using the *summary* attribute of the model, we can see how many parameters we will need to optimize in order to train the output layer.\n"]},{"cell_type":"code","execution_count":19,"id":"25b95f54-b4f5-4ff5-b9f7-59bd877fc6f2","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," resnet50 (Functional)       (None, 2048)              23587712  \n","                                                                 \n"," dense (Dense)               (None, 2)                 4098      \n","                                                                 \n","=================================================================\n","Total params: 23,591,810\n","Trainable params: 4,098\n","Non-trainable params: 23,587,712\n","_________________________________________________________________\n"]}],"source":["model.summary()"]},{"cell_type":"markdown","id":"d174115c-5697-4575-a241-c2d6a67b25cf","metadata":{},"source":["Next we compile our model using the **adam** optimizer.\n"]},{"cell_type":"code","execution_count":20,"id":"e28af4b7-b584-47d9-a996-f6e1c880e2cf","metadata":{},"outputs":[],"source":["model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"]},{"cell_type":"markdown","id":"9eb49523-42bf-43ec-9932-122c88162abf","metadata":{},"source":["Before we are able to start the training process, with an ImageDataGenerator, we will need to define how many steps compose an epoch. Typically, that is the number of images divided by the batch size. Therefore, we define our steps per epoch as follows:\n"]},{"cell_type":"code","execution_count":21,"id":"bb075ee5-15d6-4e6c-83e1-8bc4b9c9b927","metadata":{},"outputs":[],"source":["steps_per_epoch_training = len(train_generator)\n","steps_per_epoch_validation = len(validation_generator)\n","num_epochs = 2"]},{"cell_type":"markdown","id":"e940f178-9402-4b49-a92d-5d49c5ce2d84","metadata":{},"source":["Finally, we are ready to start training our model. Unlike a conventional deep learning training were data is not streamed from a directory, with an ImageDataGenerator where data is augmented in batches, we use the **fit_generator** method.\n"]},{"cell_type":"code","execution_count":null,"id":"6b2a03e7-2a37-424c-ade0-fb9030bd4fa6","metadata":{},"outputs":[],"source":["fit_history = model.fit_generator(\n","    train_generator,\n","    steps_per_epoch=steps_per_epoch_training,\n","    epochs=num_epochs,\n","    validation_data=validation_generator,\n","    validation_steps=steps_per_epoch_validation,\n","    verbose=1,\n",")"]},{"cell_type":"markdown","id":"f5ecefc5-ddd9-4a55-82e0-c2d85da6cfb6","metadata":{},"source":["Now that the model is trained, you are ready to start using it to classify images.\n"]},{"cell_type":"markdown","id":"219a8c0f-7272-4344-86b1-b48a94bf8d06","metadata":{},"source":["Since training can take a long time when building deep learning models, it is always a good idea to save your model once the training is complete if you believe you will be using the model again later. You will be using this model in the next module, so go ahead and save your model.\n"]},{"cell_type":"code","execution_count":null,"id":"6886e4e0-12c1-4f92-9131-bb65e9448da1","metadata":{},"outputs":[],"source":["model.save('./data/classifier_resnet_model.h5')"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["!rm -rf data/"]},{"cell_type":"markdown","id":"5811996e-a9d9-4f55-b217-91b706c2b540","metadata":{},"source":["Now, you should see the model file *classifier_resnet_model.h5* apprear in the left directory pane.\n"]},{"cell_type":"markdown","id":"1532efc6-f5b6-4436-8ffb-87e3ce4e2759","metadata":{},"source":["### Thank you for completing this lab!\n","\n","This notebook was created by Alex Aklson. I hope you found this lab interesting and educational.\n"]},{"cell_type":"markdown","id":"45157c39-1105-452b-bbf4-b2b9a072bbf4","metadata":{},"source":["This notebook is part of a course on **Coursera** called *AI Capstone Project with Deep Learning*. If you accessed this notebook outside the course, you can take this course online by clicking [here](https://cocl.us/DL0321EN_Coursera_Week3_LAB1).\n"]},{"cell_type":"markdown","id":"7c602219-d860-4c13-8d58-3b09e597c467","metadata":{},"source":["\n","## Change Log\n","\n","|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n","|---|---|---|---|\n","| 2020-09-18  | 2.0  | Shubham  |  Migrated Lab to Markdown and added to course repo in GitLab |\n","| 2023-01-03  | 3.0  | Artem |  Updated the file import section|\n","\n"]},{"cell_type":"markdown","id":"9c4e8fd7-5aac-4264-a473-c6b80ac193b9","metadata":{},"source":["<hr>\n","\n","Copyright &copy; 2020 [IBM Developer Skills Network](https://cognitiveclass.ai/?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/).\n"]}],"metadata":{"kernelspec":{"display_name":"tensorflow-env","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
